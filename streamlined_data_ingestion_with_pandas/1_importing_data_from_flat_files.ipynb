{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "parental-hartford",
   "metadata": {},
   "source": [
    "## Get data from CSVs\n",
    "\n",
    "In this exercise, you'll create a data frame from a CSV file. The United States makes available CSV files containing tax data by ZIP or postal code, allowing us to analyze income information in different parts of the country. We'll focus on a subset of the data, `vt_tax_data_2016.csv`, which has select tax statistics by ZIP code in Vermont in 2016.\n",
    "\n",
    "To load the data, you'll need to import the `pandas` library, then read `vt_tax_data_2016.csv` and assign the resulting data frame to a variable. Then we'll have a look at the data.\n",
    "\n",
    "Instructions\n",
    "\n",
    "1. Import the `pandas` library as `pd`.\n",
    "2. Use `read_csv()` to load `vt_tax_data_2016.csv` and assign it to the variable `data`.\n",
    "3. View the first few lines of the data frame with the `head()` method. This code has been written for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "classical-frontier",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATEFIPS</th>\n",
       "      <th>STATE</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>agi_stub</th>\n",
       "      <th>N1</th>\n",
       "      <th>mars1</th>\n",
       "      <th>MARS2</th>\n",
       "      <th>MARS4</th>\n",
       "      <th>PREP</th>\n",
       "      <th>N2</th>\n",
       "      <th>...</th>\n",
       "      <th>N10300</th>\n",
       "      <th>A10300</th>\n",
       "      <th>N85530</th>\n",
       "      <th>A85530</th>\n",
       "      <th>N85300</th>\n",
       "      <th>A85300</th>\n",
       "      <th>N11901</th>\n",
       "      <th>A11901</th>\n",
       "      <th>N11902</th>\n",
       "      <th>A11902</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>111580</td>\n",
       "      <td>85090</td>\n",
       "      <td>14170</td>\n",
       "      <td>10740</td>\n",
       "      <td>45360</td>\n",
       "      <td>130630</td>\n",
       "      <td>...</td>\n",
       "      <td>53660</td>\n",
       "      <td>50699</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10820</td>\n",
       "      <td>9734</td>\n",
       "      <td>88260</td>\n",
       "      <td>138337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>82760</td>\n",
       "      <td>51960</td>\n",
       "      <td>18820</td>\n",
       "      <td>11310</td>\n",
       "      <td>35600</td>\n",
       "      <td>132950</td>\n",
       "      <td>...</td>\n",
       "      <td>74340</td>\n",
       "      <td>221146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12820</td>\n",
       "      <td>20029</td>\n",
       "      <td>68760</td>\n",
       "      <td>151729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>46270</td>\n",
       "      <td>19540</td>\n",
       "      <td>22650</td>\n",
       "      <td>3620</td>\n",
       "      <td>24140</td>\n",
       "      <td>91870</td>\n",
       "      <td>...</td>\n",
       "      <td>44860</td>\n",
       "      <td>266097</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10810</td>\n",
       "      <td>24499</td>\n",
       "      <td>34600</td>\n",
       "      <td>90583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>30070</td>\n",
       "      <td>5830</td>\n",
       "      <td>22190</td>\n",
       "      <td>960</td>\n",
       "      <td>16060</td>\n",
       "      <td>71610</td>\n",
       "      <td>...</td>\n",
       "      <td>29580</td>\n",
       "      <td>264678</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7320</td>\n",
       "      <td>21573</td>\n",
       "      <td>21300</td>\n",
       "      <td>67045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>39530</td>\n",
       "      <td>3900</td>\n",
       "      <td>33800</td>\n",
       "      <td>590</td>\n",
       "      <td>22500</td>\n",
       "      <td>103710</td>\n",
       "      <td>...</td>\n",
       "      <td>39170</td>\n",
       "      <td>731963</td>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12500</td>\n",
       "      <td>67761</td>\n",
       "      <td>23320</td>\n",
       "      <td>103034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATEFIPS STATE  zipcode  agi_stub      N1  mars1  MARS2  MARS4   PREP  \\\n",
       "0         50    VT        0         1  111580  85090  14170  10740  45360   \n",
       "1         50    VT        0         2   82760  51960  18820  11310  35600   \n",
       "2         50    VT        0         3   46270  19540  22650   3620  24140   \n",
       "3         50    VT        0         4   30070   5830  22190    960  16060   \n",
       "4         50    VT        0         5   39530   3900  33800    590  22500   \n",
       "\n",
       "       N2  ...  N10300  A10300  N85530  A85530  N85300  A85300  N11901  \\\n",
       "0  130630  ...   53660   50699       0       0       0       0   10820   \n",
       "1  132950  ...   74340  221146       0       0       0       0   12820   \n",
       "2   91870  ...   44860  266097       0       0       0       0   10810   \n",
       "3   71610  ...   29580  264678       0       0       0       0    7320   \n",
       "4  103710  ...   39170  731963      40      24       0       0   12500   \n",
       "\n",
       "   A11901  N11902  A11902  \n",
       "0    9734   88260  138337  \n",
       "1   20029   68760  151729  \n",
       "2   24499   34600   90583  \n",
       "3   21573   21300   67045  \n",
       "4   67761   23320  103034  \n",
       "\n",
       "[5 rows x 147 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV and assign it to the variable data\n",
    "data = pd.read_csv('vt_tax_data_2016.csv')\n",
    "\n",
    "# View the first few lines of data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-delhi",
   "metadata": {},
   "source": [
    "## Get data from other flat files\n",
    "\n",
    "While CSVs are the most common kind of flat file, you will sometimes find files that use different delimiters. `read_csv()` can load all of these with the help of the `sep` keyword argument. By default, `pandas` assumes that the separator is a comma, which is why we do not need to specify `sep` for CSVs.\n",
    "\n",
    "The version of Vermont tax data here is a tab-separated values file (TSV), so you will need to use `sep` to pass in the correct delimiter when reading the file. Remember that tabs are represented as `\\t`. Once the file has been loaded, the remaining code groups the `N1` field, which contains income range categories, to create a chart of tax returns by income category.\n",
    "\n",
    "Instructions\n",
    "\n",
    "1. Import `pandas` with the alias `pd`.\n",
    "2. Load `vt_tax_data_2016.tsv`, making sure to set the correct delimiter with the `sep` keyword argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "divine-headline",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4d2a35de5df5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'agi_stub'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import pandas with the alias pd\n",
    "import pandas as pd\n",
    "\n",
    "# Load TSV using the sep keyword argument to set delimiter\n",
    "# data = pd.read_csv('vt_tax_data_2016.tsv', sep='\\t')\n",
    "data = pd.read_csv('vt_tax_data_2016.csv')\n",
    "\n",
    "# Plot the total number of tax returns by income group\n",
    "counts = data.groupby('agi_stub').N1.sum()\n",
    "counts.plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-newsletter",
   "metadata": {},
   "source": [
    "## Import a subset of columns\n",
    "\n",
    "The Vermont tax data contains 147 columns describing household composition, income sources, and taxes paid by ZIP code and income group. Most analyses don't need all these columns. In this exercise, you will create a data frame with fewer variables using `read_csv()`s `usecols` argument.\n",
    "\n",
    "Let's focus on household composition to see if there are differences by geography and income level. To do this, we'll need columns on income group, ZIP code, tax return filing status (e.g., single or married), and dependents. The data uses codes for variable names, so the specific columns needed are in the instructions.\n",
    "\n",
    "`pandas` has already been imported as `pd`.\n",
    "\n",
    "Instructions\n",
    "\n",
    "1. Create a list of columns to use: `zipcode`, `agi_stub` (income group), `mars1` (number of single households), `MARS2` (number of households filing as married), and `NUMDEP` (number of dependents).\n",
    "2. Create a data frame from `vt_tax_data_2016.csv` that uses only the selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "relevant-basin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zipcode</th>\n",
       "      <th>mars1</th>\n",
       "      <th>MARS2</th>\n",
       "      <th>NUMDEP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agi_stub</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1439444</td>\n",
       "      <td>170320</td>\n",
       "      <td>28480</td>\n",
       "      <td>52490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1439444</td>\n",
       "      <td>104000</td>\n",
       "      <td>37690</td>\n",
       "      <td>64660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1439444</td>\n",
       "      <td>39160</td>\n",
       "      <td>45390</td>\n",
       "      <td>47330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1439444</td>\n",
       "      <td>11670</td>\n",
       "      <td>44410</td>\n",
       "      <td>37760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1439444</td>\n",
       "      <td>7820</td>\n",
       "      <td>67750</td>\n",
       "      <td>60730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1439444</td>\n",
       "      <td>1210</td>\n",
       "      <td>16340</td>\n",
       "      <td>16300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          zipcode   mars1  MARS2  NUMDEP\n",
       "agi_stub                                \n",
       "1         1439444  170320  28480   52490\n",
       "2         1439444  104000  37690   64660\n",
       "3         1439444   39160  45390   47330\n",
       "4         1439444   11670  44410   37760\n",
       "5         1439444    7820  67750   60730\n",
       "6         1439444    1210  16340   16300"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create list of columns to use\n",
    "cols = ['zipcode', 'agi_stub', 'mars1', 'MARS2', 'NUMDEP']\n",
    "\n",
    "# Create data frame from csv using only selected columns\n",
    "data = pd.read_csv('vt_tax_data_2016.csv', usecols=cols)\n",
    "\n",
    "# View counts of dependents and tax returns by income level\n",
    "data.groupby('agi_stub').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-replacement",
   "metadata": {},
   "source": [
    "## Import a file in chunks\n",
    "\n",
    "When working with large files, it can be easier to load and process the data in pieces. Let's practice this workflow on the Vermont tax data.\n",
    "\n",
    "The first 500 rows have been loaded as `vt_data_first500`. You'll get the next 500 rows. To do this, you'll employ several keyword arguments: `nrows` and `skiprows` to get the correct records, `header` to tell `pandas` the data does not have column names, and `names` to supply the missing column names. You'll also want to use the `list()` function to get column names from `vt_data_first500` to reuse.\n",
    "\n",
    "`pandas` has been imported as `pd`.\n",
    "\n",
    "Instructions\n",
    "\n",
    "1. Use `nrows` and `skiprows` to make a data frame, `vt_data_next50`, with the next 500 rows.\n",
    "2. Set the `header` argument so that `pandas` knows there is no header row.\n",
    "3. Name the columns in `vt_data_next500` by supplying a list of `vt_data_first500`'s columns to the `names` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "solved-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "vt_data_first500 = pd.read_csv('vt_tax_data_2016.csv', nrows=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "miniature-engine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATEFIPS</th>\n",
       "      <th>STATE</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>agi_stub</th>\n",
       "      <th>N1</th>\n",
       "      <th>mars1</th>\n",
       "      <th>MARS2</th>\n",
       "      <th>MARS4</th>\n",
       "      <th>PREP</th>\n",
       "      <th>N2</th>\n",
       "      <th>...</th>\n",
       "      <th>N10300</th>\n",
       "      <th>A10300</th>\n",
       "      <th>N85530</th>\n",
       "      <th>A85530</th>\n",
       "      <th>N85300</th>\n",
       "      <th>A85300</th>\n",
       "      <th>N11901</th>\n",
       "      <th>A11901</th>\n",
       "      <th>N11902</th>\n",
       "      <th>A11902</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>111580</td>\n",
       "      <td>85090</td>\n",
       "      <td>14170</td>\n",
       "      <td>10740</td>\n",
       "      <td>45360</td>\n",
       "      <td>130630</td>\n",
       "      <td>...</td>\n",
       "      <td>53660</td>\n",
       "      <td>50699</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10820</td>\n",
       "      <td>9734</td>\n",
       "      <td>88260</td>\n",
       "      <td>138337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>82760</td>\n",
       "      <td>51960</td>\n",
       "      <td>18820</td>\n",
       "      <td>11310</td>\n",
       "      <td>35600</td>\n",
       "      <td>132950</td>\n",
       "      <td>...</td>\n",
       "      <td>74340</td>\n",
       "      <td>221146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12820</td>\n",
       "      <td>20029</td>\n",
       "      <td>68760</td>\n",
       "      <td>151729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>46270</td>\n",
       "      <td>19540</td>\n",
       "      <td>22650</td>\n",
       "      <td>3620</td>\n",
       "      <td>24140</td>\n",
       "      <td>91870</td>\n",
       "      <td>...</td>\n",
       "      <td>44860</td>\n",
       "      <td>266097</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10810</td>\n",
       "      <td>24499</td>\n",
       "      <td>34600</td>\n",
       "      <td>90583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>30070</td>\n",
       "      <td>5830</td>\n",
       "      <td>22190</td>\n",
       "      <td>960</td>\n",
       "      <td>16060</td>\n",
       "      <td>71610</td>\n",
       "      <td>...</td>\n",
       "      <td>29580</td>\n",
       "      <td>264678</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7320</td>\n",
       "      <td>21573</td>\n",
       "      <td>21300</td>\n",
       "      <td>67045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>39530</td>\n",
       "      <td>3900</td>\n",
       "      <td>33800</td>\n",
       "      <td>590</td>\n",
       "      <td>22500</td>\n",
       "      <td>103710</td>\n",
       "      <td>...</td>\n",
       "      <td>39170</td>\n",
       "      <td>731963</td>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12500</td>\n",
       "      <td>67761</td>\n",
       "      <td>23320</td>\n",
       "      <td>103034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATEFIPS STATE  zipcode  agi_stub      N1  mars1  MARS2  MARS4   PREP  \\\n",
       "0         50    VT        0         1  111580  85090  14170  10740  45360   \n",
       "1         50    VT        0         2   82760  51960  18820  11310  35600   \n",
       "2         50    VT        0         3   46270  19540  22650   3620  24140   \n",
       "3         50    VT        0         4   30070   5830  22190    960  16060   \n",
       "4         50    VT        0         5   39530   3900  33800    590  22500   \n",
       "\n",
       "       N2  ...  N10300  A10300  N85530  A85530  N85300  A85300  N11901  \\\n",
       "0  130630  ...   53660   50699       0       0       0       0   10820   \n",
       "1  132950  ...   74340  221146       0       0       0       0   12820   \n",
       "2   91870  ...   44860  266097       0       0       0       0   10810   \n",
       "3   71610  ...   29580  264678       0       0       0       0    7320   \n",
       "4  103710  ...   39170  731963      40      24       0       0   12500   \n",
       "\n",
       "   A11901  N11902  A11902  \n",
       "0    9734   88260  138337  \n",
       "1   20029   68760  151729  \n",
       "2   24499   34600   90583  \n",
       "3   21573   21300   67045  \n",
       "4   67761   23320  103034  \n",
       "\n",
       "[5 rows x 147 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATEFIPS</th>\n",
       "      <th>STATE</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>agi_stub</th>\n",
       "      <th>N1</th>\n",
       "      <th>mars1</th>\n",
       "      <th>MARS2</th>\n",
       "      <th>MARS4</th>\n",
       "      <th>PREP</th>\n",
       "      <th>N2</th>\n",
       "      <th>...</th>\n",
       "      <th>N10300</th>\n",
       "      <th>A10300</th>\n",
       "      <th>N85530</th>\n",
       "      <th>A85530</th>\n",
       "      <th>N85300</th>\n",
       "      <th>A85300</th>\n",
       "      <th>N11901</th>\n",
       "      <th>A11901</th>\n",
       "      <th>N11902</th>\n",
       "      <th>A11902</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>5356</td>\n",
       "      <td>2</td>\n",
       "      <td>180</td>\n",
       "      <td>120</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>250</td>\n",
       "      <td>...</td>\n",
       "      <td>170</td>\n",
       "      <td>497</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>76</td>\n",
       "      <td>130</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>5356</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>150</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>460</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>142</td>\n",
       "      <td>50</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>5356</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>110</td>\n",
       "      <td>...</td>\n",
       "      <td>50</td>\n",
       "      <td>471</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>5356</td>\n",
       "      <td>5</td>\n",
       "      <td>80</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>170</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>2229</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>531</td>\n",
       "      <td>30</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>5356</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATEFIPS STATE  zipcode  agi_stub   N1  mars1  MARS2  MARS4  PREP   N2  \\\n",
       "0         50    VT     5356         2  180    120     40      0    90  250   \n",
       "1         50    VT     5356         3   80     50     40      0    40  150   \n",
       "2         50    VT     5356         4   50      0     40      0    40  110   \n",
       "3         50    VT     5356         5   80     20     50      0    60  170   \n",
       "4         50    VT     5356         6    0      0      0      0     0    0   \n",
       "\n",
       "   ...  N10300  A10300  N85530  A85530  N85300  A85300  N11901  A11901  \\\n",
       "0  ...     170     497       0       0       0       0      50      76   \n",
       "1  ...      80     460       0       0       0       0      40     142   \n",
       "2  ...      50     471       0       0       0       0       0       0   \n",
       "3  ...      80    2229       0       0       0       0      30     531   \n",
       "4  ...       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   N11902  A11902  \n",
       "0     130     212  \n",
       "1      50     148  \n",
       "2      30      87  \n",
       "3      30     246  \n",
       "4       0       0  \n",
       "\n",
       "[5 rows x 147 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create data frame of next 500 rows with labeled columns\n",
    "vt_data_next500 = pd.read_csv('vt_tax_data_2016.csv', \n",
    "                              nrows=500,\n",
    "                              skiprows=500,\n",
    "                              header=None,\n",
    "                              names=list(vt_data_first500))\n",
    "\n",
    "# View the Vermont data frames to confirm they're different\n",
    "display(vt_data_first500.head())\n",
    "display(vt_data_next500.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-benefit",
   "metadata": {},
   "source": [
    "## Specify data types\n",
    "\n",
    "When loading a flat file, `pandas` infers the best data type for each column. Sometimes its guesses are off, particularly for numbers that represent groups or qualities instead of quantities.\n",
    "\n",
    "Looking at the data dictionary for `vt_tax_data_2016.csv` reveals two such columns. The `agi_stub` column contains numbers that correspond to income categories, and `zipcode` has 5-digit values that should be strings -- treating them as integers means we lose leading 0s, which are meaningful. Let's specify the correct data types with the `dtype` argument.\n",
    "\n",
    "`pandas` has been imported for you as `pd`.\n",
    "\n",
    "Instructions\n",
    "\n",
    "1. Load `vt_tax_data_2016.csv` with no arguments and view the data frame's `dtypes` attribute. Note the data types of `zipcode` and `agi_stub`.\n",
    "2. Create a dictionary, `data_types`, specifying that `agi_stub` is `\"category\"` data and `zipcode` is string data.\n",
    "3. Reload the CSV with the `dtype` argument and the dictionary to set the correct column data types.\n",
    "4. View the data frame's `dtypes` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "atomic-diabetes",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STATEFIPS     int64\n",
       "STATE        object\n",
       "zipcode       int64\n",
       "agi_stub      int64\n",
       "N1            int64\n",
       "              ...  \n",
       "A85300        int64\n",
       "N11901        int64\n",
       "A11901        int64\n",
       "N11902        int64\n",
       "A11902        int64\n",
       "Length: 147, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load csv with no additional arguments\n",
    "data = pd.read_csv('vt_tax_data_2016.csv')\n",
    "\n",
    "# Print the data types\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "considerable-customer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STATEFIPS       int64\n",
       "STATE          object\n",
       "zipcode        object\n",
       "agi_stub     category\n",
       "N1              int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dict specifying data types for agi_stub and zipcode\n",
    "data_types = {'agi_stub': 'category',\n",
    "              'zipcode': str}\n",
    "\n",
    "# Load csv using dtype to set correct data types\n",
    "data = pd.read_csv('vt_tax_data_2016.csv', dtype=data_types)\n",
    "\n",
    "# Print data types of resulting frame\n",
    "data.dtypes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-instrumentation",
   "metadata": {},
   "source": [
    "## Set custom NA values\n",
    "\n",
    "Part of data exploration and cleaning consists of checking for missing or NA values and deciding how to account for them. This is easier when missing values are treated as their own data type and there are `pandas` functions that specifically target such NA values. `pandas` automatically treats some values as missing, but we can pass additional NA indicators with the `na_values` argument. Here, you'll do this to ensure that invalid ZIP codes in the Vermont tax data are coded as NA.\n",
    "\n",
    "`pandas` has been imported as `pd`.\n",
    "\n",
    "Instructions\n",
    "\n",
    "1. Create a dictionary, `null_values`, specifying that `0`s in the `zipcode` column should be considered NA values.\n",
    "2. Load `vt_tax_data_2016.csv`, using the `na_values` argument and the dictionary to make sure invalid ZIP codes are treated as missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "atomic-helmet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATEFIPS</th>\n",
       "      <th>STATE</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>agi_stub</th>\n",
       "      <th>N1</th>\n",
       "      <th>mars1</th>\n",
       "      <th>MARS2</th>\n",
       "      <th>MARS4</th>\n",
       "      <th>PREP</th>\n",
       "      <th>N2</th>\n",
       "      <th>...</th>\n",
       "      <th>N10300</th>\n",
       "      <th>A10300</th>\n",
       "      <th>N85530</th>\n",
       "      <th>A85530</th>\n",
       "      <th>N85300</th>\n",
       "      <th>A85300</th>\n",
       "      <th>N11901</th>\n",
       "      <th>A11901</th>\n",
       "      <th>N11902</th>\n",
       "      <th>A11902</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>111580</td>\n",
       "      <td>85090</td>\n",
       "      <td>14170</td>\n",
       "      <td>10740</td>\n",
       "      <td>45360</td>\n",
       "      <td>130630</td>\n",
       "      <td>...</td>\n",
       "      <td>53660</td>\n",
       "      <td>50699</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10820</td>\n",
       "      <td>9734</td>\n",
       "      <td>88260</td>\n",
       "      <td>138337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>82760</td>\n",
       "      <td>51960</td>\n",
       "      <td>18820</td>\n",
       "      <td>11310</td>\n",
       "      <td>35600</td>\n",
       "      <td>132950</td>\n",
       "      <td>...</td>\n",
       "      <td>74340</td>\n",
       "      <td>221146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12820</td>\n",
       "      <td>20029</td>\n",
       "      <td>68760</td>\n",
       "      <td>151729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>46270</td>\n",
       "      <td>19540</td>\n",
       "      <td>22650</td>\n",
       "      <td>3620</td>\n",
       "      <td>24140</td>\n",
       "      <td>91870</td>\n",
       "      <td>...</td>\n",
       "      <td>44860</td>\n",
       "      <td>266097</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10810</td>\n",
       "      <td>24499</td>\n",
       "      <td>34600</td>\n",
       "      <td>90583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>30070</td>\n",
       "      <td>5830</td>\n",
       "      <td>22190</td>\n",
       "      <td>960</td>\n",
       "      <td>16060</td>\n",
       "      <td>71610</td>\n",
       "      <td>...</td>\n",
       "      <td>29580</td>\n",
       "      <td>264678</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7320</td>\n",
       "      <td>21573</td>\n",
       "      <td>21300</td>\n",
       "      <td>67045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>39530</td>\n",
       "      <td>3900</td>\n",
       "      <td>33800</td>\n",
       "      <td>590</td>\n",
       "      <td>22500</td>\n",
       "      <td>103710</td>\n",
       "      <td>...</td>\n",
       "      <td>39170</td>\n",
       "      <td>731963</td>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12500</td>\n",
       "      <td>67761</td>\n",
       "      <td>23320</td>\n",
       "      <td>103034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>VT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>9620</td>\n",
       "      <td>600</td>\n",
       "      <td>8150</td>\n",
       "      <td>0</td>\n",
       "      <td>7040</td>\n",
       "      <td>26430</td>\n",
       "      <td>...</td>\n",
       "      <td>9600</td>\n",
       "      <td>894432</td>\n",
       "      <td>3350</td>\n",
       "      <td>4939</td>\n",
       "      <td>4990</td>\n",
       "      <td>20428</td>\n",
       "      <td>3900</td>\n",
       "      <td>93123</td>\n",
       "      <td>2870</td>\n",
       "      <td>39425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATEFIPS STATE  zipcode  agi_stub      N1  mars1  MARS2  MARS4   PREP  \\\n",
       "0         50    VT      NaN         1  111580  85090  14170  10740  45360   \n",
       "1         50    VT      NaN         2   82760  51960  18820  11310  35600   \n",
       "2         50    VT      NaN         3   46270  19540  22650   3620  24140   \n",
       "3         50    VT      NaN         4   30070   5830  22190    960  16060   \n",
       "4         50    VT      NaN         5   39530   3900  33800    590  22500   \n",
       "5         50    VT      NaN         6    9620    600   8150      0   7040   \n",
       "\n",
       "       N2  ...  N10300  A10300  N85530  A85530  N85300  A85300  N11901  \\\n",
       "0  130630  ...   53660   50699       0       0       0       0   10820   \n",
       "1  132950  ...   74340  221146       0       0       0       0   12820   \n",
       "2   91870  ...   44860  266097       0       0       0       0   10810   \n",
       "3   71610  ...   29580  264678       0       0       0       0    7320   \n",
       "4  103710  ...   39170  731963      40      24       0       0   12500   \n",
       "5   26430  ...    9600  894432    3350    4939    4990   20428    3900   \n",
       "\n",
       "   A11901  N11902  A11902  \n",
       "0    9734   88260  138337  \n",
       "1   20029   68760  151729  \n",
       "2   24499   34600   90583  \n",
       "3   21573   21300   67045  \n",
       "4   67761   23320  103034  \n",
       "5   93123    2870   39425  \n",
       "\n",
       "[6 rows x 147 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dict specifying that 0s in zipcode are NA values\n",
    "null_values = {'zipcode': 0}\n",
    "\n",
    "# Load csv using na_values keyword argument\n",
    "data = pd.read_csv('vt_tax_data_2016.csv', \n",
    "                   na_values=null_values)\n",
    "\n",
    "# View rows with NA ZIP codes\n",
    "data[data['zipcode'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-synthesis",
   "metadata": {},
   "source": [
    "## Skip bad data\n",
    "\n",
    "In this exercise you'll use `read_csv()` parameters to handle files with bad data, like records with more values than columns. By default, trying to import such files triggers a specific error, `pandas.io.common.CParserError`.\n",
    "\n",
    "Some lines in the Vermont tax data here are corrupted. In order to load the good lines, we need to tell `pandas` to skip errors. We also want `pandas` to warn us when it skips a line so we know the scope of data issues.\n",
    "\n",
    "`pandas` has been imported as `pd`. The exercise code will try to read the file. If there is a `pandas.io.common.CParserError`, the code in the `except` block will run.\n",
    "\n",
    "Instructions\n",
    "\n",
    "1. Try to import the file `vt_tax_data_2016_corrupt.csv` without any keyword arguments.\n",
    "2. Import `vt_tax_data_2016_corrupt.csv` with the `error_bad_lines` parameter set to skip bad records.\n",
    "3. Update the import with the `warn_bad_lines` parameter set to issue a warning whenever a bad record is skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sonic-michigan",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas.io.common' has no attribute 'CParserError'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-913c9f035682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Import the CSV without any keyword arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vt_tax_data_2016_corrupt.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vt_tax_data_2016_corrupt.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-913c9f035682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mexcept\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCParserError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Your data contained rows that could not be parsed.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas.io.common' has no attribute 'CParserError'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Import the CSV without any keyword arguments\n",
    "    data = pd.read_csv('vt_tax_data_2016_corrupt.csv')\n",
    "  \n",
    "    # View first 5 records\n",
    "    print(data.head())\n",
    "  \n",
    "except pd.io.common.CParserError:\n",
    "    print('Your data contained rows that could not be parsed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Import CSV with error_bad_lines set to skip bad records\n",
    "    data = pd.read_csv('vt_tax_data_2016_corrupt.csv', \n",
    "                     error_bad_lines=False)\n",
    "  \n",
    "    # View first 5 records\n",
    "    print(data.head())\n",
    "  \n",
    "except pd.io.common.CParserError:\n",
    "    print('Your data contained rows that could not be parsed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-growing",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Set warn_bad_lines to issue warnings about bad records\n",
    "    data = pd.read_csv('vt_tax_data_2016_corrupt.csv', \n",
    "                       error_bad_lines=False, \n",
    "                       warn_bad_lines=True)\n",
    "  \n",
    "    # View first 5 records\n",
    "    print(data.head())\n",
    "  \n",
    "except pd.io.common.CParserError:\n",
    "    print('Your data contained rows that could not be parsed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
